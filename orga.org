- Vorhersage für Benzinpreise je Tankstelle
  - 1 Monat Vorausschau
  - gegebene Uhrzeit
  - Clustering
    - Autobahn?
    - Berufsverkehr?
    - Pampa?
- Tankstrategie
  - Fixed-Path-Gas-Station-Problem
  - leerer Tank an gegebener Tkst
  - pro Tkst auf Route:
    - berechne getankte Menge pro Tkst --> minimiere Kosten
- Deadline Impl: Ende Nov
- 2 Services: Vorhersage, Routing

* TODOS
** TODO Paper lesen
** TODO Data Exploration
** TODO KNN, KMeans: testen, ob gleiche kategorien gefunden werden, die wir finden (Autobahn etc)
* Database
- install postgresql
[[https://wiki.archlinux.org/index.php/PostgreSQL]]
#+begin_src sh
pacman -Syu postgresql
systemctl start postgresql
sudo -u postgres -i
initdb --locale $LANG -E UTF8 -D '/var/lib/postgres/data'
#+end_src
- create db
#+begin_src sh
sudo -u postgres -i
createdb infocup
#+end_src
- connect to db and create tables
#+begin_src sh
pgcli -d infocup
#+end_src
#+begin_src sql
create table stations (
  id integer primary key,
  station_name varchar(255),
  brand varchar(255),
  street varchar(255),
  house_number varchar(255),
  zip_code varchar(5),
  city varchar(255),
  lat float,
  lon float);
create table prices (
  id serial primary key,
  time_stamp timestamp,
  price integer,
  station_id integer references stations);
#+end_src
- import data
#+begin_src sh
python database.py
#+end_src
** OSM Daten
#+begin_note
Um duplicated keys zu vermeiden, werden zuerst alle Daten in einzelne Files gefiltert und anschließen gemerged.
#+end_note
- download germany-latest.osm.pbf von [[http://download.geofabrik.de/europe/germany.html]] hier verwendet: Stand 18.10.2017 14:14
- mv =germany-latest.osm.pbf= nach =WeitereDaten=
- install osmfilter, osmconvert und osmosis
#+begin_src sh
pacaur -S osmfilter osmconvert osmosis
#+end_src
- convert pbf nach o5m für schnellere bearbeitung
#+begin_src sh
osmconvert germany-latest.osm.pbf -o=germany.o5m
#+end_src
- Straßen: [[http://wiki.openstreetmap.org/wiki/Attributierung_von_Stra%C3%9Fen_in_Deutschland]]
- regionale Grenzen: [[http://wiki.openstreetmap.org/wiki/DE:Grenze]]
- filter data
    - Autobahn: =--keeprelations="network=BAB"=
    #+begin_src sh
    osmfilter germany.o5m --keep= --keep-relations="network=BAB" > autobahn.osm
    #+end_src
    - Schnellstrassen: =--keep-ways="highway=trunk oneway=yes"=
    #+begin_src sh
    osmfilter germany.o5m --keep= --keep-ways="highway=trunk oneway=yes" > schnellstr.osm
    #+end_src
    - Bundesstrassen: =--keep-ways="highway=primary"=
    #+begin_src sh
    osmfilter germany.o5m --keep= --keep-ways="highway=primary" > bundesstr.osm
    #+end_src
    - Grenzen Bundeslaender: =--keep-relations="boundary=administrative admin_level=4"=
    #+begin_src sh
    osmfilter germany.o5m --keep= --keep-relations="boundary=administrative admin_level=4" > laender.osm
    #+end_src
    - Kreisgrenzen: =--keep-relations="boundary=administrative admin_level=6"=
    #+begin_src sh
    osmfilter germany.o5m --keep= --keep-relations="boundary=administrative admin_level=6" > kreise.osm
    #+end_src
- sort data
#+begin_quote
Merges the contents of two data sources together.

Note that this task requires both input streams to be sorted first by type then by id.
#+end_quote
#+begin_src sh
osmosis --read-xml autobahn.osm --log-progress --sort --write-xml autobahn_sorted.osm
osmosis --read-xml schnellstr.osm --log-progress --sort --write-xml schnellstr_sorted.osm
osmosis --read-xml bundesstr.osm --log-progress --sort --write-xml bundesstr_sorted.osm
osmosis --read-xml laender.osm --log-progress --sort --write-xml laender_sorted.osm
osmosis --read-xml kreise.osm --log-progress --sort --write-xml kreise_sorted.osm
#+end_src
- clean up intermediate
#+begin_src sh
rm autobahn.osm schnellstr.osm bundesstr.osm laender.osm kreise.osm
#+end_src
- merge files
#+begin_src sh
osmosis --read-xml autobahn_sorted.osm --read-xml schnellstr_sorted.osm --merge --sort --write-xml merge.osm
osmosis --read-xml merge.osm --read-xml bundesstr_sorted.osm --merge --sort --write-xml merge2.osm
osmosis --read-xml merge2.osm --read-xml laender_sorted.osm --merge --sort --write-xml merge3.osm
osmosis --read-xml merge3.osm --read-xml kreise_sorted.osm --merge --sort --write-xml mergefinal.osm
#+end_src
- clean up intermediate
#+begin_src sh
rm merge.osm merge2.osm merge3.osm
#+end_src
- install postgis extension, create schema
#+begin_src sh
pacman -Syu postgis
pgcli -d infocup
#+end_src
#+begin_src sql
create extension postgis;
create extension hstore;
#+end_src
#+begin_src sh
psql -d infocup -f /usr/share/osmosis/script/pgsnapshot_schema_0.6.sql
psql -d infocup -f /usr/share/osmosis/script/pgsnapshot_schema_0.6_linestring.sql 
#+end_src
- read o5m, write postgis
#+begin_src sh
osmosis --truncate-pgsql database=infocup
osmosis --read-xml mergefinal.osm --log-progress --write-pgsql database=infocup
#+end_src
#+begin_src pseudo
make coffee
#+end_src
- add ref col to ways for better grouping
#+begin_src sql
alter table ways add ref varchar(255);
update ways set ref = tags -> 'ref';
#+end_src

*** Notes
- The data in Open Street Map database is stored in a gcs with units decimal degrees & datum of wgs84. (EPSG: 4326)
- The Open Street Map tiles and the WMS webservice, are in the projected coordinate system that is based on the wgs84 datum. (EPSG 3857)
- table relations enthält die relationen wie zb A 81
  - table relations_members mapt ways und nodes auf relations (bei Autobahnen nur ways)
  - ways haben eine linestring column mit der die geometrie angezeigt werden kann [[http://arthur-e.github.io/Wicket/sandbox-gmaps3.html]]
** Vacations
- create table
#+begin_src sql
create table vacations (
  id serial primary key,
  state varchar(30) not null,
  type varchar(20) not null,
  start_date date not null,
  end_date date not null);
#+end_src
- import data
#+begin_src sh
python database.py vacations
#+end_src

** Pendler
- herkunft: [[https://statistik.arbeitsagentur.de/Navigation/Statistik/Statistische-Analysen/Interaktive-Visualisierung/Pendleratlas/Pendleratlas-Nav]]
- create table
#+begin_src sql
create table commuters (
  region varchar(5) primary key,
  name varchar(255) not null,
  from_region jsonb not null,
  to_region jsonb notnull);
#+end_src
- import data
#+begin_src sh
python database.py commuters
#+end_src


** Prices sampled
#+begin_src sql
insert into prices_sampled (time_stamp, price, station_id) (select ts, 0, s.id from generate_series('2015-01-01', '2017-10-30', interval '1 hour') as ts, (select id from stations) as s)
#+end_src
#+begin_src sql
update prices_sampled as ps set price = (select price from prices as p where p.time_stamp <= ps.time_stamp and p.station_id = ps.station_id order by p.time_stamp desc limit 1)
#+end_src
#+begin_src sql
alter table stations add column min_ts timestamp;
alter table stations add column max_ts timestamp;
#+end_src
#+begin_src sql
delete from prices_sampled as p where p.station_id in (select id from stations as s where min_ts is null)
#+end_src
#+begin_src sql
delete from prices_sampled as p where p.time_stamp < (select s.min_ts from stations as s where s.id = p.station_id limit 1);
delete from prices_sampled as p where p.time_stamp > (select s.max_ts from stations as s where s.id = p.station_id limit 1);
#+end_src
#+begin_src sql
create index on prices_sampled (station_id, time_stamp)
where time_stamp between '2015-01-01' and '2017-09-18'
#+end_src

** OSM-Labels für Tankstellen
Die Koordinaten für die Tankstellen sind wie die OSM Daten in WGS84 (EPSRG: 4326) angegeben. PostGIS unterstützt 2
Typen für Koordinaten: Geography und Geometry. Geography bietet die besseren Ergebnisse, unterstützt aber weniger
Funktionen und benötigt mehr Rechenleistung als Geometry. Geometry sollte für unsere Zwecke ausreichen. WGS84 wird für
GPS verwendet und die Einheit ist Grad. Dies bedeutet, dass PostGIS Funktionen wie ST_Distance auch Grad als Ergebnis
verwenden. Eine Umrechnung in ein anderes Koordinatensystem, dass auf Metern basiert wäre daher sinnvoll. EPSG 4839 ist
ein Koordinatensystem für Europa, dass Meter verwendet und eine Genauigkeit von 1m bietet. Dies sollte für Geo-Label
ausreichen.

[[https://epsg.io/4839]]

#+begin_src sql
alter table stations add column location geometry(Point, 4326);
update stations as s set location = st_setsrid(st_makepoint(s.lon, s.lat), 4326);
alter table stations add column location_4839 geometry(Point, 4839);
update stations as s set location_4839 = st_transform(s.location, 4839);
create table autobahn (
  id serial primary key,
  rel_id bigint,
  geom geometry(multilinestring, 4326),
  tags hstore);
insert into autobahn (rel_id, tags)
    (select id,
        tags
     from relations
     where tags -> 'network' = 'BAB');
#+end_src

- execute update_autobahn script

#+BEGIN_SRC sql
alter table autobahn add column geom_4839 geometry(multilinestring, 4839);
update autobahn set geom_4839 = st_transform(geom, 4839);
alter table stations add column dist_abahn double precision, add abahn_id int;
create index stations_location_gix on stations using gist (location_4839);
create index autobahn_geom_gix on autobahn using gist (geom_4839);

with distances as (
  SELECT distinct on (s.id)
   a.id as aid,
   s.id as sid,
   st_distance(a.geom_4839, s.location_4839) as dist
  from autobahn as a
    inner join stations as s on st_dwithin(a.geom_4839, s.location_4839, 5000)
  ORDER BY s.id, a.id, st_distance(a.geom_4839, s.location_4839)
)
update stations
set abahn_id = d.aid, dist_abahn = d.dist
from distances as d
where id = d.sid;
#+END_SRC


* Papers
** ECONOMETRIC MODELS FOR OIL PRICE FORECASTING: A CRITICAL SURVEY
[[https://drive.google.com/open?id=0B0zREaCR-EN4c3BTSnVYU3pHWFk]]

In the last two years the price of oil and its fluctua-
tions have reached levels never recorded in the his-
tory of international oil markets. In 2007, the West
Texas Intermediate (WTI) oil price, one of the most
important  benchmarks  for  crude  oil  prices,  aver-
aged around 72 $/b, while in 2008 the WTI price was
around  100  $/b, with  an  increase  of  nearly  38  per-
cent  over  the  previous  year.  Within  the  past  six
months, WTI  daily  spot  prices  ranged  from  almost
150 $/b in early July to about 30 $/b towards the end
of 2008.
The  determinants  of  past, current, and  future  levels
of the price of oil and its fluctuations have been the
subject of analysis by academics and energy experts,
given  the  relevance  of  crude  oil  in  the  worldwide
economy. Although the share of liquid fuels in mar-
keted  world  energy  consumption  is  expected  to
decline  from  37  percent  in  2005  to  33  percent  in
2030, and projected high oil prices will induce many
consumers to switch from liquid fuels when feasible,
oil will remain the most important source of energy,
and  liquid  fuel  consumption  is  expected  to  increase
at an average annual rate of 1.2 percent from 2005 to
2030 (EIA 2008).
The crucial question of whether oil prices will rise in
the  future  or  will  decline  again  is  timely. According
to EIA (2009), for example, under current economic
and world crude oil supply assumptions, WTI prices
are expected to average 43 $/b in 2009 and 55 $/b in
2010. The possibility of a milder recession or a faster
economic  recovery, lower  non-OPEC  production  in
response to current low oil prices and financial mar-
ket constraints, and more aggressive action to lower
production  by  OPEC  countries  could  result  in  a
faster  and  stronger  recovery  in  oil  prices.  Conse-
quently, it  is  extremely  important  for  economists  to
provide accurate answers to the complex problem of
forecasting oil prices.
This  study  aims  at  investigating  the  existing  econo-
metric literature on forecasting oil prices. In particu-
lar, we (i) develop a taxonomy of econometric mod-
els  for  oil  price  forecasting;  (ii)  provide  a  critical
interpretation  of  the  different  methodologies;  and
(iii) offer a comprehensive interpretation and justifi-
cation  of  the  heterogeneous  empirical  findings  in
published oil price forecasts. The paper is structured
as  follows:  we  first  introduce  the  historical  frame-
work  which  is  necessary  to  understand  oil  price
dynamics. The  following  section  discusses  and  criti-
cally evaluates the different econometric models for
oil  price  forecasting  proposed  in  the  literature.
Finally we comment on alternative criteria for eval-
uating  and  comparing  different  forecasting  models
for oil prices.

** Forecasting the Nominal Brent Oil Price with VARs—One Model Fits All?
[[https://drive.google.com/open?id=0B0zREaCR-EN4ZmdkZXdTUENrRFk]]

We carry out an ex post assessment of popular models used to forecast oil prices and propose
a host of alternative VAR models based on traditional global macroeconomic and oil market
aggregates. While the exact specification of VAR models for nominal oil price prediction is
still open to debate, the bias and underprediction in futures and random walk forecasts are
larger across all horizons in relation to a large set of VAR specifications. The VAR forecasts
generally have the smallest average forecast errors and the highest accuracy, with most
specifications outperforming futures and random walk forecasts for horizons up to two years.
This calls for caution in reliance on futures or the random walk for forecasting, particularly
for near term predictions. Despite the overall strength of VAR models, we highlight some
performance instability, with small alterations in specifications, subsamples or lag lengths
providing widely different forecasts at times. Combining futures, random walk and VAR
models for forecasting have merit for medium term horizons.

** Characteristic-Based Clustering for Time Series Data
[[https://drive.google.com/open?id=0B0zREaCR-EN4LUROUDJrcTZoVUk]]

With the growing importance of time series clustering research, particularly for similarity searches
amongst long time series such as those arising in medicine or finance, it is critical for us to find a way to
resolve the outstanding problems that make most clustering methods impractical under certain circumstances.
When the time series is very long, some clustering algorithms may fail because the very notation of similarity
is dubious in high dimension space; many methods cannot handle missing data when the clustering is based
on a distance metric.
This paper proposes a method for clustering of time series based on their structural characteristics. Unlike
other alternatives, this method does not cluster point values using a distance metric, rather it clusters based
on global features extracted from the time series. The feature measures are obtained from each individual
series and can be fed into arbitrary clustering algorithms, including an unsupervised neural network algorithm,
self-organizing map, or hierarchal clustering algorithm.
Global measures describing the time series are obtained by applying statistical operations that best capture
the underlying characteristics: trend, seasonality, periodicity, serial correlation, skewness, kurtosis, chaos,
nonlinearity, and self-similarity. Since the method clusters using extracted global measures, it reduces the
dimensionality of the time series and is much less sensitive to missing or noisy data. We further provide a
search mechanism to find the best selection from the feature set that should be used as the clustering inputs.
The proposed technique has been tested using benchmark time series datasets previously reported for time
series clustering and a set of time series datasets with known characteristics. The empirical results show that
our approach is able to yield meaningful clusters. The resulting clusters are similar to those produced by other
methods, but with some promising and interesting variations that can be intuitively explained with knowledge
of the global characteristics of the time series.

** Clustering of large time series datasets
[[https://drive.google.com/open?id=0B0zREaCR-EN4RGJCOEw2eHhRbkk]]

Time series clustering is a very effective approach in discovering valuable information in various systems such as
finance, embedded bio-sensor and genome. However, focusing on the efficiency and scalability of these algorithms to deal with
time series data has come at the expense of losing the usability and effectiveness of clustering. In this paper a new multi-step
approach is proposed to improve the accuracy of clustering of time series data. In the first step, time series data are clustered
approximately. Then, in the second step, the built clusters are split into sub-clusters. Finally, sub-clusters are merged in the third
step. In contrast to existing approaches, this method can generate accurate clusters based on similarity in shape in very large
time series datasets. The accuracy of the proposed method is evaluated using various published datasets in different domains.
